\section{Introduction}

\emph{Rule lists}, also called decision lists, are one-sided decision trees.

\section{Related work}

\citep{rivest:1987}

\citep{LethamRuMcMa15}

\citep{YangRuSe16}

\citep{garofalakis:2000-kdd,garofalakis:2000-sigkdd,garofalakis:2003}

\section{A branch-and-bound framework for optimizing rule lists}

\subsection{Rule lists for binary classification}

We restrict our setting to binary classification,
where rule lists are Boolean functions;
this framework is straightforward to generalize to multi-class classification.
%
Let us denote training data by~${\{(x_n, y_n)\}_{n=1}^N}$,
where~${x_n \in \{0, 1\}^d}$ are binary features and~${y_n \in \{0, 1\}}$ are labels.

A rule list~$\RL$ of length~$k \ge 0$ is defined by a $(k+1)$-tuple consisting of~$k$
association rules followed by a default rule~$d$.
%
An association rule~${p \rightarrow q}$ is an implication corresponding to the
conditional statement, ``if~$p$, then~$q$.''
%
In our setting, an antecedent~$p$ is a Boolean assertion that evaluates to either
true or false for each datum~$x_n$, and a consequent~$q$ is a label prediction.
%
For example,~${(x_{i, 1} = 0) \wedge (x_{i, 3} = 1) \rightarrow (y_i = 1)}$
is an association rule that could appear in a rule list.
%
The number of conditions in an antecedent is its cardinality;
the antecedent in the previous example has a cardinality of two.
%
The final default rule~$d$ in a rule list can be thought of as a special
association rule whose antecedent simply asserts true.

A rule list classifies a datum~$x_n$ by providing the predicted label given
by the consequent of the first rule whose antecedent is true for~$x_n$.
%
We say that this rule captures~$x_n$ in the context of the rule list;
note that this rule can be the default rule.
%
Let a rule list's prefix~$\Prefix$ be a $k$-tuple of its antecedents.
%
We say that a prefix of length~$k$ captures those data that are captured
by one of its~$k$ association rules; for a rule list with prefix~$\Prefix$,
data not captured by~$\Prefix$ are classified by the default rule.
%
We say that a prefix~$\Prefix'$ of length~$k'$ starts with prefix~$\Prefix$ of
length~$k \le k'$ if the first~$k$ association rules of~$\Prefix'$ are given by~$\Prefix$.

Finally, given training data, a prefix of antecedents implies a rule list
whose label predictions of the corresponding consequents and of the default rule
are empirically determined to minimize the number misclassification errors made
by the rule list on the training data.
%
In the remainder of our presentation, whenever we refer to a rule list with a
particular prefix, we implicitly assume these empirically determined label predictions.

\subsection{Branch-and-bound optimization framework}

We define a simple loss function for a rule list~$\RL$ of length~$k$:
\begin{align}
\ell(\RL) = m(\RL) + c k.
\label{eq:objective}
\end{align}
The two terms correspond to the misclassification error~$m(\RL)$
and a regularization term that penalizes longer rule lists.
%
$m(\RL)$~is the fraction of training data whose labels are
incorrectly predicted by~$\RL$.
%
In our setting, the regularization parameter~${c \ge 0}$ is a small constant;
\eg ${c = 0.01}$ can be thought of as adding a penalty equivalent to misclassifying~$1\%$
of data when increasing a decision rule's length by one association rule.
%
We will sometimes abuse notation and write~$\ell(\Prefix)$ in place of~$\ell(\RL)$,
where~$\RL$ is the rule list with prefix~$\Prefix$ and label predictions set by training data.

Our objective has structure amenable to global optimization via a branch-and-bound framework.
%
In particular, we can decompose the misclassification error into two contributions, 
\begin{align}
m(\RL) = m(\Prefix) + m(d),
\end{align}
corresponding to mistakes made by the prefix and default rule, respectively.
%
Eliminating the latter error term gives a lower bound~$b(\Prefix)$ on the objective,
\begin{align}
b(\Prefix) \equiv m(\Prefix) + c k \le \ell(\RL),
\label{eq:lower-bound}
\end{align}
which is actually a lower bound on the objective of \emph{any} rule list
of length~$k$ or longer whose prefix starts with~$\Prefix$.
%
Notice that if a prefix~$\Prefix'$ starts with~$\Prefix$, then~${m(\Prefix') \ge m(\Prefix)}$
because~$\Prefix'$ makes the same mistakes as~$\Prefix$, and possibly additional mistakes.
%
Therefore,~${b(\Prefix') \ge b(\Prefix)}$.
%
A sequence of rule lists formed by sequentially appending association rules
to the end of a prefix thus has a corresponding sequence of
monotonically increasing objective lower bounds.
%
This is precisely the structure required and exploited by branch-and-bound.

Specifically, the lower bound tells us when we can prune the state space.
%
While executing branch-and-bound, we keep track of the best (smallest) known objective~$\ell^*$.
%
If we encounter a prefix~$\Prefix$ with lower bound at least as large as~$\ell^*$,
\begin{align}
b(\Prefix) \ge \ell^*,
\end{align}
then we needn't consider any prefix that starts with~$\Prefix$.

\subsection{Additional pruning bounds}

Our problem has rich structure beyond the objective lower bound in~\eqref{eq:lower-bound}.
%
In this section, we enumerate a series of additional bounds that combine to yield
aggressive pruning opportunities throughout the execution of our branch-and-bound algorithm.

\subsubsection{Lower bound with one-step lookahead}

We begin with an immediate consequence of~\eqref{eq:lower-bound}.
%
Given a prefix~$\Prefix$ of length~$k$, any longer prefix~$\Prefix'$ that starts
with~$\Prefix$ has length at least~${k+1}$.
%
Thus, even if we find that~${b(\Prefix) < \ell^*}$, we can still prune all longer
prefixes~$\Prefix'$ if
\begin{align}
b(\Prefix) + c = m(\Prefix) + c (k + 1) \ge \ell^*,
\end{align}
where $\ell^*$ is the best known objective.

\subsubsection{Upper bound on prefix length}

At any point during branch-and-bound execution, the best known objective~$\ell^*$
implies an upper bound on the maximum prefix length we might still have to consider:
\begin{align}
L \equiv \left\lfloor \frac{\ell^*}{c} \right\rfloor \le \left\lfloor \frac{1}{c} \right\rfloor.
\end{align}
Furthermore, if we ever encounter a perfect rule list~$\RL$ of length~$K$,
\ie with~${\ell(\RL) = m(\RL) + c = c}$, then we only have to consider shorter prefixes:
\begin{align}
L = K - 1.
\end{align}

\subsubsection{Lower bound on per-rule accurate support}

Suppose~$\RL^*$ is the optimal rule list, and rule~$A$ is in~$\RL^*$.
%
Let~$\RL_{<A}^*$ refer to the portion of~$\RL^*$ preceding rule~$A$,
let~$\RL_A^*$ denote rule~$A$ in the context of~$\RL^*$, and let~$\RL_{>A}^*$
refer to the portion of~$\RL^*$ following rule~$A$, including the default rule.
%
We can decompose~$\ell(\RL^*)$ into three contributions,
corresponding to the above three components of~$\RL^*$:
\begin{align}
\ell^* = \ell(\RL^*) = \ell(\RL_{<A}^*) + (m(\RL_A^*) + c) + \ell(\RL_{>A}^*).
\end{align}
Now consider the rule list~$\RL$ derived from~$\RL^*$ by deleting rule~$A$.
%
Let~$\RL_{<A}$ and~$\RL_{>A}$ refer to the portions of~$\RL$
preceding and following where rule~$A$ had been, respectively.
%
Notice that~$\RL_{<A}$ behaves the same as~$\RL_{<A}^*$,
but~$\RL_{>A}$ may behave differently from~$\RL_{>A}^*$.
%
Specifically,~$\RL_{>A}$ captures all data captured by~$\RL_{>A}^*$, as well as
none, some, or all data captured by~$\RL_A^*$.
%
The worst possible loss~$\ell(\RL)$ would occur if all the data
captured by~$\RL_A^*$ were now misclassified by~$\RL_{>A}$.
%
Let~$s(\RL_A^*)$ denote the normalized support of~$\RL_A^*$,
\ie the fraction of data captured by rule~$A$.
%
For~$\RL^*$ to be optimal, we must have~${\ell(\RL^*) < \ell(\RL)}$, which implies that
\begin{align}
m(\RL_A^*) + c + \ell(\RL_{>A}^*) < \ell(\RL_{>A}) \le \ell(\RL_{>A}^*) + s(\RL_A^*).
\end{align}
Rearranging gives
\begin{align}
c < s(\RL_A^*) - m(\RL_A^*),
\label{eq:min-capture-correct}
\end{align}
therefore the regularization parameter~$c$ provides a lower bound on the expression
on the right, which is the fraction of data correctly classified by rule~$A$.
%
Thus, we can prune a prefix~$\Prefix$ if any of its antecedents do not capture and
correctly classify more than a fraction~$c$ of data, even if~${b(\Prefix) < \ell^*}$.

\subsubsection{Lower bound on per-rule support}

A requirement of the previous bound~\eqref{eq:min-capture-correct}
is that for a rule list~$\RL^*$ to be optimal, every rule~$A$ in~$\RL^*$ must
capture more than a fraction~$c$ of data:
\begin{align}
c < s(\RL_A^*).
\label{eq:min-capture}
\end{align}
Thus, we can prune a prefix if any of its rules do not capture at least
a fraction~$c$ of data.
%
This lower bound is easy to check and a sub-condition of~\eqref{eq:min-capture-correct},
thus checking it first can accelerate pruning.

\subsubsection{Insufficient per-rule support due to rule dominance}

Let us say that rule~$A$ dominates rule~$B$ if the data captured by~$B$ is a subset of the data captured by~$A$.
%
Rule~$B$ should never follow rule~$A$ in a rule list because it will never capture additional data;
this scenario is a special case where~\eqref{eq:min-capture} immediately applies.
%
More precisely, if~$\RL$ is a rule list that contains rule~$A$ and doesn't contain rule~$B$,
and~$\RL'$ is derived from~$\RL$ by inserting rule~$B$ anywhere after~$A$, then
\begin{align}
\ell(\RL') = \ell(\RL) + c \ge \ell(\RL).
\end{align}
By considering rule semantics, it is easy to think of common situations
leading to dominance relationships between rules.
%
For example, if rules~$A$ and~$B$ have antecedents~${(x_1 = 0)}$
and~${(x_1 = 0) \wedge (x_2 = 1)}$, then rule~$A$ dominates rule~$B$.
%
%Similar to symmetry-aware pruning, we achieve this by restricting how we grow a prefix, as informed by a hash map rdict that maps a rule R_i to a set of rules T_i = {R_k} such that R_i dominates every R_k in T_i. When growing a prefix that ends with rule R_i, we only append a rule R_k if it is not in T_i. Notice that the intersection of S_i and T_i is the empty set, thus the mappings represented by cdict and rdict can easily be combined into a single mapping.

\subsubsection{Propagation of insufficient (accurate) support}

Suppose we grow a rule list by appending rule~$A$ not already in the rule list,
\ie by adding an antecedent to its prefix~$\Prefix$.
%
Now suppose rule~$A$ in the context of this new rule list~$\RL$ has
insufficient accurate support~\eqref{eq:min-capture-correct},
with insufficient support~\eqref{eq:min-capture} being a special case.
%
It follows that for \emph{any} rule list~$\RL'$ that places rule~$A$ after
a set of rules whose antecedents include in any order the antecedents in~$\Prefix$,
then rule~$A$ has insufficient accurate support in the context of~$\RL'$.
%
Restating this using earlier notation, if~${s(\RL_A) \le c}$, then
\begin{align}
s(\RL'_A) \le c,
\end{align}
where~$s(\RL_A)$ is the fraction of data captured by rule~$A$ in the context of rule list~$\RL$.
%
For example, consider a rule list~$\RL'$ derived from~$\RL$ that holds rule~$A$ fixed
but permutes any of the other (earlier) antecedents given by those in~$\Prefix$.
%
The permutation of~$\Prefix$ captures the same data as~$\Prefix$,
therefore rule~$A$ behaves the same in both contexts,
and in particular captures and correctly classifies the same data.
%
Another rule list~$\RL''$, derived from~$\RL'$ by inserting additional rules anywhere,
maintains the invariant that rule~$A$ appears after rules with antecedents in~$\Prefix$;
it can capture and correctly classify at most as much data as rule~$A$ in~$\RL'$.
%
%A rule is rejected by a prefix if it doesn't correctly capture enough data (it must correctly capture >= c * ndata). Let Q be P's parent. If Q rejects a rule, then P will also reject that rule. This 'inheritance' of rejected rules only depends on which data are captured by Q, and doesn't actually depend on the order of rules in Q. Let S be the set of rules formed from (K-1) rules of P, in any order. P inherits rejected rules from any elements of S. Because of our symmetry-based garbage collection of prefixes equivalent up to a permutation, there are at most K elements of S in the cache; we can identify these via the inverse canonical map (ICM) that maps an ordered prefix to its permutation in the cache. We thus lazily initialize the list of P's reject list of rejected rules.

\subsubsection{Permutation bound for symmetry-aware garbage collection}

If two prefixes~$\cal{P}$ and~$\cal{Q}$ are composed of the same antecedents and
equivalent up to a permutation, then they also capture the same data.
%
Their two corresponding rule lists need not yield the same objective, since the
loss function~\eqref{eq:objective} depends on rule order.
%
Obtain a prefix~$\cal{P}'$ by appending~$\cal{P}$ with some ordered list of
unique antecedents not contained in~$\cal{P}$, and~$\cal{Q}'$ by appending~$\cal{Q}$
with the same ordered list.
%
The performance of the rule list formed from~$\cal{P}'$ compared to~$\cal{P}$ will be
the same as that of the rule list formed from~$\cal{Q}'$ compared to~$\cal{Q}$, \ie
\begin{align}
\ell(\cal{P}') - \ell(\cal{P}) = \ell(\cal{Q}') - \ell(\cal{Q}).
\end{align}
Without loss of generality, suppose that~${\ell(\cal{P}) \ge \ell(\cal{Q})}$.
%
Let~$\cal{P}^*$ be the optimal prefix that starts with~$\cal{P}$.
%
Its counterpart~$\cal{Q}^*$ is the optimal prefix that starts with~$\cal{Q}$,
and it cannot be superior to~$\cal{P^*}$:
%
\begin{align}
\ell(\cal{P}^*) = \ell(\cal{Q^*}) + (\ell(\cal{P}) - \ell(\cal{Q})) \ge \ell(\cal{Q^*}).
\end{align}
%
Thus, we can prune~$\cal{Q}$;
in our implementation, we call this symmetry-aware garbage collection.
%
Since a prefix of length~$k$ belongs to an equivalence class of~$k!$ prefixes
equivalent up to permutation, this garbage collection dramatically prunes the search space.

For example, consider a state space defined by rule lists up to length~$K$
formed from a set of~$M$ available rules.
%
We can think of our problem as finding the optimal $k$-permutation of the~$M$ rules,
where~${k \le K}$.
%
This na\"ively gives a state space of size
\begin{align}
\sum_{k=0}^K P(M, k) = \sum_{k=0}^K \frac{M!}{(M - k)!}
%= 1 + M + M(M-1) + M(M-1)(M-2) + \dots + M(M-1)\cdots (M-k+1)
\end{align}
where~${P(M, k)}$ denotes the number of $k$-permutations of~$M$.

Now consider a breadth-first exploration of the state space,
where after evaluating prefixes of length~$k$, we only keep a single best prefix
from each set of prefixes equivalent up to a permutation.
%
For example, we start by evaluating the empty prefix,
$M$ prefixes of length~${k=1}$, and~${P(M, 2)}$ prefixes of length~${k=2}$.
%
Before proceeding to length~${k=3}$, we keep only~${C(M, 2)}$ prefixes of length~${k=2}$,
where~${C(M, k)}$ denotes the number of $k$-combinations of~$M$.
%
Now, the number of length~${k=3}$ prefixes we evaluate is~${C(M, 2) (M - 2)}$.
%
Propagating this forward reduces the (maximum) number of evaluated prefixes to
\begin{align}
1 + \sum_{k=1}^K C(M, k-1) (M - k + 1)
%= 1 + \sum_{k=1}^K {M \choose k-1}(M - k + 1)
%= 1 + \sum_{k=1}^K \frac{M! (M - k + 1)}{(k - 1)! (M - k + 1)!}
= 1 + \sum_{k=1}^K \frac{1}{(k - 1)!} \cdot \frac{M!}{(M - k)!}.
\end{align}
Pruning based on permutation symmetries thus yields significant computational savings.

For example, if~${M = 100}$ and~${K = 5}$, then the na\"ive number of prefix evaluations is
about ${9.1 \times 10^9}$, while the reduced number of evaluations is about ${3.9 \times 10^8}$,
which is smaller by a factor of about~23.
%
If~${M=1000}$ and~${K = 10}$, the number of evaluations fall from
about~${9.6 \times 10^{29}}$ to about~${2.7 \times 10^{24}}$,
which is smaller by a factor of about~360,000.
%
% ELA : someone please double-check these numbers :)

\subsubsection{Equivalence of rule lists when rules commute}

If two rules~$A$ and~$B$ capture disjoint subsets of data,
then they commute globally in the sense that any rule list where~$A$ and~$B$ are
adjacent is equivalent to another rule list where~$A$ and~$B$ swap positions.
%
More generally, a rule list containing possibly multiple, possibly overlapping
pairs of commuting rules is equivalent to any other rule list that can be generated
by swapping one or more such pairs of rules.
%
We can avoid evaluating multiple such equivalent rule lists by eliminating all but one.
%
%We achieve this by restricting how we grow a prefix (by appending a rule), as informed by a hash map cdict that maps each rule R_i to a set of rules S_i = {R_j} such that R_i commutes with every R_j in S_i and j > i. When growing a prefix that ends with rule R_i, we only append a rule R_j if it is not in S_i.

\subsubsection{Lower bound on similarity to the default rule}

Let~$\RL$ be a rule list with prefix~$\Prefix$, default rule~$d$, and objective~$\ell(\RL)$.
%
Let~$\RL'$ be a rule list with prefix~$\Prefix'$ that starts with~$\Prefix$ and adds one rule~$A$.
%
If, in the context of~$\RL'$, rule~$A$ captures all remaining data not captured by~$\Prefix$,
then rule~$A$ behaves like~$\RL$'s default rule~$d$ while incurring a penalty of~$c$,
\ie ~${\ell(\RL') = \ell(\RL) + c}$.
%
% It would never make sense to extend~$\RL'$ with more rules, since no data remains to be captured.
%
More generally, \dots
%
% ELA appears to have gotten confused here

\subsubsection{Not captured bound}

Generalization of captures, perhaps with some details to work out?
If two prefixes aren't formed from the same set of rules, then their subtrees aren't quite the same.

\begin{itemize}
\item other permutation bounds
\item similarity bounds
\item not-too-many incorrect bound
\end{itemize}

\subsection{Cache data structure}
\label{sec:cache}

Our cache is a trie.

\subsection{Symmetry-aware data structure}

\subsection{Scheduling policies}

\begin{itemize}
\item breadth-first
\item depth-first
\item something based on greedy
\item (curiosity, lower bound, optimization) $\times$ (priority queue, something like Thompson sampling)
\item optimistic
\end{itemize}

\subsection{Large-scale optimization}

\subsection{System}

\section{Experiments}

\begin{itemize}

\item Value of the objective for us and a few other algorithms (CART, C4.5, CBA, CMAR/CPAR, C5.0 \dots), and algorithm runtimes 

\item Show the effect of each ``piece'' at a time, \eg curiosity -- run it without each in turn and show the difference in either quality of solution or runtime or amount of memory, size of cache or queue

\item Fraction of search space eliminated over time

\item Size of queue over time

\item Total number of things placed in queue (over time?)

\item Objective function value over time -- this also gives an upper bound on the remaining search space -- and also, possibly, the minimum lower bound in the queue over time

\item Some characterization of the number of solutions close to optimal (plot number of suboptimal solutions vs amount of suboptimality, removing permutations)

\item Some example rule lists to show how interpretable they are. Potentially we could display equally optimal rule lists that look very different from each other. 

\end{itemize}

\section{Conclusions}

\subsubsection*{Acknowledgments}

E.A. is supported by the Miller Institute for Basic Research in Science, University of California, Berkeley.

\bibliography{refs}
\bibliographystyle{abbrvnat}
