\section{Introduction}

As machine learning continues to grow in importance for socially-important decisions, the interpretability of predictive models remains a crucial problem. Our aim is to build models that are highly predictive but in which each step of the model's decision making process can also be investigated by humans. To achieve this, we use rule lists, also known as decision lists, which are lists composed of if-then statements. This structure allows for predictive models that also can be easily interpreted; the rules give a reason for each prediction.

The problem of how to construct rule lists, or more generally, decision trees, has been in existence at least 30 years \cite{rivest:1987,CART,C5.0}. The vast majority of approaches use greedy splitting techniques \cite{rivest:1987,CART,C5.0,etc}. More modern techniques have used Bayesian analysis, either to find a locally optimal solution \cite{BART} or to actually explore the search space \citep{LethamRuMcMa15, YangRuSe16}. These more modern approaches achieve high accuracy while also managing to run reasonably quickly. However, despite the apparent accuracy of the rule lists generated by these algorithms, there is no way to determine if the generated rule list is optimal or how close to optimal the rule lists is.

Optimality is important because there are societal implications for a lack of optimality. The Pro-Publica article on the COMPAS recidivism prediction tool \citep{LarsonMaKiAn16} is one example. It highlights a case where a black-box, proprietary predictive model is being used for recidivism prediction. \citep{LarsonMaKiAn16}  show that the COMPAS scores are racially biased, but since the model is not transparent, no one (outside of the creators of COMPAS) can determine the reason or extent of the bias. Nor can anyone determine the true reasons for any particular prediction. It was determined that a transparent model would not be sufficiently accurate for recidivism prediction, thus a more accurate, black box model would have to suffice. We wondered whether there was indeed no possible transparent model that would suffice. To determine the answer to that question, one would need to solve a computationally hard problem, namely to find a transparent model that is actually optimal, with a certificate of optimality, among a particular pre-determined class of models. That way one could say, with certainty, whether a transparent model (from this class of models) with sufficient accuracy exists for this problem, before resorting to black box models.

In our framework, we consider the class of rule lists created from pre-mined frequent itemsets. The rule list must be assembled from these itemsets to minimize a regularized risk functional, $R$. This is a hard discrete optimization problem. A brute force solution to find the rule list that minimizes $R$ would be computationally prohibitive due to the exponential number of possible rule lists. This, however, is a worst case bound that is not realized in practical settings. For realistic cases, it is possible to solve fairly large cases of this problem to optimality, with the careful use of algorithms, data structures, and bit-vector manipulation.

We develop specialized tools from the field of discrete optimization and artificial intelligence, and in particular, a special branch-and-cut algorithm, called "??", that provides (1) the optimal solution, (2) near-optimal solutions, (3) a certificate of optimality, and if optimality is not achieved, the distance between the current solution and optimality. Because of the certificate of optimality, this method can also be used to investigate how close other models (e.g., models provided by greedy algorithms) are to optimality. In particular, we can investigate if the rule lists from probabilistic approaches are nearly optimal or whether those approaches sacrifice too much accuracy to gain speed for constructing the model.

Within its branch-and-cut procedure, ??? maintains an upper bound on the maximum value of $R$ that each incomplete rule list can achieve. This allows it to prune an incomplete rule list (and every possible extension) if the bound is worse than the accuracy of the best rule list that we've already looked at. The use of careful bounding techniques leads to massive pruning of the search space of potential rule lists. We continue to consider incomplete and complete rule lists until we have either looked at every rule list or eliminated it from consideration. Thus, the end of execution leaves us with the optimal rule list, the close-to-optimal rule lists, and a certificate of optimality.

The efficacy of ??? depends on how much of the search space our bounds allow us to prune. The upper bound on $R$ must thus be as tight as reasonably possible. The bound we maintain throughout the calculation is a minimum of several bounds, that come in three categories. The first category of bounds are those intrinsic to the rules themselves. This category includes bounds stating that each rule must capture sufficient data; if not, the rule list is provably non-optimal. The second type of bound compares an upper bound on the value of $R$ to that of the current best solution. This allows us to exclude parts of the search space that could never be better than our current solution. Finally, our last type of bound is based on comparing incomplete rule lists that capture the same data and pursuing only the more accurate option. This last class of bounds is especially important -- without our use of a novel \textit{symmetry-aware map}, we are unable to solve most problems of reasonable scale. This symmetry-aware map keeps track of the best accuracy for all the permutations of a given incomplete rule list.

In order to keep track of all of these bounds for each rule list, we implemented a modified trie that we call a prefix tree. Each node in the prefix tree represents an individual rule; thus, each path in the tree represents a rule list where the final node in the path contains the metrics about that rule list. This tree structure facilities the use of multiple different selection algorithms including breadth-first search, a priority queue based on a custom curiosity function, and a stochastic selection process. In addition, we are able to limit the number of nodes in the tree and thereby achieve a way of tuning space-time tradeoffs in a robust manner. We propose that this tree structure is a useful way of organizing the generation of rule lists and could allow future implementations of ??? to be easily parallelized.

We evaluated BBRL??? on a number of publicly available datasets, and have made code for our algorithm and experiments publicly available. Our metric of success was 10-fold cross validated prediction accuracy on a subset of the data. These datasets involve hundreds of rules and hundreds or thousands of observations. BBRL??? is generally able to find the optimal rule list in a matter of seconds and certify it within about 10 minutes. We show that we are able to achieve better out-of-sample accuracy on these datasets than the popular greedy algorithms, CART or C5.0.

This algorithm is designed for solving large (not massive) problems, where interpretability and certifiable optimality is important. A key example of the type of problems this algorithm is useful for is recidivism prediction. Thus, we work on the dataset released by Northpointe, who designed the proprietary COMPAS algorithm. We show that, at least on this sample of the data, it is possible to produce an certifiably optimal interpretable rule list that achieves the same accuracy as a random forest applied to this dataset. We thus see no reason why a proprietary algorithm should be used for recidivism prediction.

\section{Related work}

BRL and SBRL

	Recent work in the field of decision lists has focused on the creation of probabilistic decision lists that generate a posterior distribution over the space of potential decision lists\citep{LethamRuMcMa15,YangRuSe16}. These methods achieve good accuracy while maintaining a small execution time. In addition, these methods improve on existing methods such as CART or C5.0 by optimizing over the global space of decision lists as opposed to searching for rules greedily and getting stuck at local optima. We take the same approach towards optimizing over the global search space, though we donâ€™t use probabilistic techniques. In addition, we use the rule mining framework from \citep{LethamRuMcMa15} to generate the rules for our data sets. \citep{YangRuSe16} builds on \citep{LethamRuMcMa15} by placing bounds on the search space and creating a high performance bit vector manipulation library. We use that bit vector manipulation library to perform our computations, and add additional bounds to further prune the search space.

Garofalakis

Efficient Algorithms for Constructing Decision Trees with Constraints, Scalable Data Mining with Model Constraints, Building Decision Trees with Constraints

	Our use of a branch and bound technique has also been applied to decision tree generation methods. \citep{garofalakis:2000-kdd} created an algorithm to generate more interpretable decision trees by allowing one to constrain the size of the decision tree. \citep{garofalakis:2000-kdd} uses branch-and-bound to constrain the size of the search space and limit the eventual size of the decision tree. During tree construction, \citep{garofalakis:2000-kdd} bounds the possible MDL cost of every different split at a given node. If every split at that node is more expensive than the actual cost of the current subtree, then that node can be pruned. In this way, they were able to prune the tree while constructing it instead of just constructing the tree and then pruning at the end.

ProPublica

	Certain problems require that the model used to solve that problem be interpretable as well as accurate. \citep{LarsonMaKiAn16} examines the problem of predicting recidivism and shows that a black box model, specifically the COMPAS score from the company Northpointe, has racially biased prediction. Black defendants are misclassified at a higher risk for recidivism than in actuality, while white defendants are misclassified at a lower risk. The model which produces the COMPAS scores is a black box algorithm which is not interpretable, and therefore the model does not provide a way for human input to correct for these racial biases. Our model produces similar accuracies to the logistic regression and COMPAS scores from \citep{LarsonMaKiAn16} while maintaining its interpretability.

\citep{rivest:1987}

\citep{LethamRuMcMa15}

\citep{YangRuSe16}

\citep{garofalakis:2000-kdd,garofalakis:2000-sigkdd,garofalakis:2003}

\input{framework}

\input{bounds}

\section{Implementation architecture}

We present an architecture for executing our branch-and-bound algorithm,
consisting of a cache, a queue that is associated with a search policy,
and, optionally, a symmetry-aware map.
%
First, we describe the cache, our primary data structure~(\S\ref{sec:cache});
it is organized as a prefix tree and supports the incremental computations,
detailed in~\S\ref{sec:incremental}, that are central to our approach.
%
Second, we describe the queue and search policy~(\S\ref{sec:queue}).
%
Like the queue in Algorithm~\ref{alg:branch-and-bound},
our queue keeps track of which prefixes to evaluate during execution.
%
The policy for selecting a prefix from the queue to evaluate next,
and thus also the natural queue data structure, depend on
the search policy employed for exploring the space of rule lists
%
Next, we describe the symmetry-aware map~(\S\ref{sec:map}),
which enables garbage collection of prefixes eliminated by the
equivalent support bound in Theorem~\ref{sec:equivalent}.
%
While we present the map as an optional component of our architecture,
our calculations in~\S\ref{sec:permutation-counting}
and experiments in~\S\ref{sec:experiments} demonstrate that
it is critical for efficient and practical algorithm performance.
%
Finally, we summarize an artifact we implemented~(\S\ref{sec:system}),
which we evaluate in~\S\ref{sec:experiments}.

\subsection{Prefix tree cache for incremental computation}
\label{sec:cache}

We maintain a cache to support incremental computation.
%
Our cache is organized as a prefix tree, which is also known as a trie.
%

\subsection{Queue and search policies}
\label{sec:queue}

Different search policies suggest different natural queue data structures.

\begin{itemize}
\item breadth-first
\item depth-first
\item something based on greedy
\item (curiosity, lower bound, optimization) $\times$ (priority queue, something like Thompson sampling)
\item optimistic
\end{itemize}

\subsection{Map data structure for symmetry-aware garbage collection}
\label{sec:map}

%\subsection{Large-scale optimization}

\subsection{System}
\label{sec:system}

\section{Preliminary experiments}
\label{sec:experiments}

Notes:
\begin{itemize}

\item Measurements over multiple executions (corresponding to ten folds)
should include standard deviations

\item Most figures will be generated with one particular big dataset
\end{itemize}

Probably want to integrate some measurements of the permutation map's effects
(from Margo's email):
\begin{itemize}
\item How many times do we look up an item in the hash map that is not in the tree?

\item When we do look up an item, how many times do we end up discarding
      something we would have been unable to discard had we not done this.

\item How is the hash map growing over time?

\item How is the tree growing over time?
\end{itemize}

Figures:
\begin{itemize}

\item Figure~\ref{fig:comparison}, Comparison with other methods:
Test error for us and a few other algorithms
(CART, C4.5, CBA, CMAR/CPAR, C5.0, Ripper, \dots),
as a function of sparsity (number of rules) over 10 folds, for one big dataset (box plots).
Also report algorithm runtimes (mean $\pm$ standard deviation over 10 folds).
Also compare accuracies to random forests and boosted decision trees.

\item Figure~\ref{fig:regularization}
Missing:  Test error as a function of regularization and sparsity
(number of rules) as a function of regularization, over 10 folds,
for one big dataset.

\item Figure~\ref{fig:ablation},  Ablation experiment:
Show the effect of each ``piece'' at a time,
run X without each in turn and show the difference in either
quality of solution or runtime or amount of memory, size of cache or queue,
where X is a specific implementation
(meaning a specific scheduling policy and node type)

\item Figure~\ref{fig:scheduling-policy}
Missing:  Some sort of comparison of different scheduling policies

\item Figure~\ref{fig:queue-cache-size-insertions} (top),
Size of cache and queue data structures as a function of wall clock time

\item Figure~\ref{fig:queue-cache-size-insertions} (bottom),
Cumulative number of things placed in queue over time

\item Figure~\ref{fig:objective}, Objective value over time,
with horizontal lines and x-ticks for CART and C4.5
%-- this also gives an upper bound on the remaining search space --
%and also, possibly, the minimum lower bound in the queue over time

\item Figure~\ref{fig:search-space},
Fraction of search space we've proven we've handled

\item Figure~\ref{fig:max-length},
Max prefix length over time (computed from objective value);

\item Figure~\ref{fig:prefix-length},
Best prefix length over time

\item Some characterization of the number of solutions close to optimal
(plot number of suboptimal solutions vs amount of suboptimality,
removing permutations)

\item Some example rule lists to show how interpretable they are.
Potentially we could display equally optimal rule lists that look
very different from each other.

\end{itemize}

\section{Conclusions}

\subsubsection*{Acknowledgments}

E.A. is supported by the Miller Institute for Basic Research in Science,
University of California, Berkeley.

\bibliography{refs}
\bibliographystyle{abbrvnat}

\begin{figure}[t!]
\begin{center}
%\includegraphics[width=0.75\textwidth]{figs/sketch-comparison.png}
\includegraphics[width=0.75\textwidth]{figs/compare-compas.pdf}
\end{center}
\caption{Comparison with other methods:
Test error for us and a few other algorithms
(CART, C4.5, CBA, CMAR/CPAR, C5.0, Ripper, \dots),
as a function of sparsity over 10 folds, for one big dataset (box plots).
Also report algorithm runtimes (mean $\pm$ standard deviation over 10 folds).}
\label{fig:comparison}
\end{figure}

\begin{figure}[t!]
\begin{center}
\end{center}
\caption{Missing:  Test error as a function of regularization and sparsity
(number of rules) as a function of regularization, over 10 folds,
for one big dataset.}
\label{fig:regularization}
\end{figure}

\begin{figure}[t!]
\begin{center}
\includegraphics[width=0.75\textwidth]{figs/sketch-ablation.png}
\end{center}
\caption{Ablation experiment:
Show the effect of each ``piece'' at a time,
run X without each in turn and show the difference in either
quality of solution or runtime or amount of memory, size of cache or queue,
where X is a specific implementation
(meaning a specific scheduling policy and node type)}
\label{fig:ablation}
\end{figure}

\begin{figure}[t!]
\begin{center}
\end{center}
\caption{Missing:  Some sort of comparison of different scheduling policies}
\label{fig:scheduling-policy}
\end{figure}

\begin{figure}[t!]
\begin{center}
%\includegraphics[width=0.65\textwidth]{figs/sketch-queue-size.png}
\includegraphics[width=0.8\textwidth]{figs/ela_compas-queue-cache-size-insertions.pdf}
\end{center}
\caption{Cache and queue data structure sizes and insertions.
%
The top plot shows the sizes of the cache and queue data structures,
as a function of wall clock time.
%
The number of nodes in the cache (solid black line) is an
upper bound on the number of elements in the physical queue
(dotted gray line), since the physical queue elements only
correspond to the cache trie data structure's leaf nodes
plus disconnected cache nodes that have been marked for deletion.
%
The queue's physical size is an upper bound on its
logical size (solid blue line), which doesn't include nodes
that have been marked for deletion.
%
The bottom plot shows the cumulative number of cache insertions,
which is equivalent to the cumulative number of queue insertions,
as a function of wall clock time.
}
\label{fig:queue-cache-size-insertions}
\end{figure}

\begin{figure}[t!]
\begin{center}
\includegraphics[width=0.8\textwidth]{figs/ela_compas-queue.pdf}
\end{center}
\caption{Logical queue composition.
}
\label{fig:queue}
\end{figure}

\begin{figure}[t!]
\begin{center}
%\includegraphics[width=0.65\textwidth]{figs/sketch-objective.png}
\includegraphics[width=0.8\textwidth]{figs/ela_compas-objective.pdf}
\end{center}
\caption{Objective value as a function of wall clock time.
%
The top plot shows the entire execution, and the bottom plot
highlights the optimization phase, from the start of execution
to the time at which the optimal value is achieved.
%
The cyan circle indicates the objective value after a single iteration,
and the magenta square indicates the time at which the optimum
is achieved, as well as its value.
%
Missing: horizontal lines and x-ticks for CART and C4.5.}
\label{fig:objective}
\end{figure}

\begin{figure}[t!]
\begin{center}
%\includegraphics[width=0.65\textwidth]{figs/sketch-search-space.png}
\includegraphics[width=0.8\textwidth]{figs/ela_compas-remaining-space.pdf}
\end{center}
\caption{The logarithm (base 10) of an upper bound
on the size of the remaining search space,
as a function of wall clock time.
%
These plots show the upper bound in
Proposition~\ref{prop:remaining-eval-coarse},
which depends on the total number of available rules,
as well as two dynamic quantities:
the current best objective value,
and the histogram of logical queue elements,
partitioned by prefix length.
%
The top plot shows the entire execution,
the middle plot highlights the optimization phase,
and the bottom plot highlights the verification phase.}
\label{fig:search-space}
\end{figure}

%\begin{figure}[t!]
%\begin{center}
%%\includegraphics[width=0.65\textwidth]{figs/sketch-max-length.png}
%\includegraphics[width=0.8\textwidth]{figs/ela-max-length-check.png}
%\end{center}
%\caption{Max prefix length over time (computed from objective value)}
%\label{fig:max-length}
%\end{figure}
%
\begin{figure}[t!]
\begin{center}
\includegraphics[width=0.8\textwidth]{figs/ela_compas-prefix-length.pdf}
\end{center}
\caption{Best prefix length over time}
\label{fig:prefix-length}
\end{figure}
