\documentclass[aoas,preprint]{imsart}
\usepackage{fullpage}
\setattribute{journal}{name}{}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}

\usepackage{graphicx,verbatim}
\usepackage[round]{natbib}
\usepackage{url}
\usepackage{amsmath,amssymb,amsthm,amsfonts}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{todonotes}
\usepackage{subfig}
\usepackage{dsfont}
\usepackage{listings}
\usepackage{comment}

\usepackage{tikz}
\usetikzlibrary{arrows}

\input{latex_macros}
\frenchspacing
\hyphenation{speed-up}

\begin{document}

\section{Introduction}

As machine learning continues to grow in importance for socially-important decisions, the interpretability of predictive models remains a crucial problem. Our aim is to build models that are highly predictive but in which each step in the model's decision making process can also be investigated by humans. To achieve this, we use rule lists, also known as decision lists, which are lists composed of if-then statements. This structure allows for predictive models that also can be easily interpreted; the rules give a reason for each prediction.

The problem of how to construct rule lists, or more generally, decision trees, has been in existence at least 30 years \cite{rivest:1987, CART, C4.5}. The vast majority of approaches use greedy splitting techniques \cite{rivest:1987, CART, C4.5,several more}. More modern techniques have used Bayesian analysis, either to find a locally optimal solution \cite{BART} or to actually explore the search space \citep{LethamRuMcMa15, YangRuSe16}. These more modern approaches achieve high accuracy while also managing to run reasonably quickly. However, despite the apparent accuracy of the rule lists generated by these algorithms, there is no way to determine if the generated rule list is optimal or how close to optimal the rule lists is.

Why is optimality important? It is because there are societal implications for lack of optimality. The Pro-publica article on the COMPAS recidivism prediction tool \cite{} is one example. It highlights a case where a black-box, proprietary predictive model is being used for recidivism prediction. The model is clearly racially biased, but since the model is not transparent, no one (outside of the creators of COMPAS) can determine the reason or extent of the bias. Nor can anyone determine the true reasons for any particular prediction. This is a case where it was determined that a transparent model would not be sufficiently accurate for recidivism prediction, thus a more accurate, black box model would suffice. This begs the question of whether indeed there is no possible transparent model that would suffice. To determine the answer to that question, one would need to solve a computationally hard problem, namely to find a transparent model that is actually optimal, with a certificate of optimality, among a particular pre-determined class of models. That way one could say, with certainty, whether a transparent model (from this class of models) with sufficient accuracy exists for this problem, before resorting to black box models.

In our framework, we consider the class of rule lists created from pre-mined frequent itemsets. The rule list must be assembled from these itemsets to minimize a regularized risk functional, $R$. This is a hard discrete optimization problem. A brute force solution to find the rule list that minimizes $R$ would be computationally prohibitive due to the exponential number of possible rule lists. This, however, is a worst case bound that is not realized in practical settings. For realistic cases, it is possible to solve fairly large cases of this problem to optimality, with the careful use of algorithms, data structures, and bit-vector manipulation.

We develop specialized tools from the field of discrete optimization and artificial intelligence, and in particular, a speciality branch-and-cut algorithm, called "??", that provides (1) the optimal solution, (2) near-optimal solutions, (3) a certificate of optimality, and if optimality is not achieved, a gap on closeness to optimality. Because of the certificate of optimality, this method can also be used to investigate how close other models (e.g., models provided by greedy algorithms) are to optimality. In particular, we can investigate if the rule lists from probabilistic approaches are nearly optimal or whether those approaches sacrifice too much accuracy to gain speed for constructing the model.

Within its branch-and-cut procedure, ??? maintains an upper bound on the maximum value of $R$ that each incomplete rule list can achieve. This allows it to prune an incomplete rule list (and every possible extension) if the bound is worse than the accuracy of the best rule list that we've already looked at. The use of careful bounding techniques leads to massive pruning of the search space of potential rule lists. We continue to consider incomplete and complete rule lists until we have either looked at every rule list or eliminated it from consideration. Thus, the end of execution leaves us with the optimal rule list, the close-to-optimal rule lists, and a certificate of optimality.

The efficacy of ??? depends on how much of the search space our bounds allow us to prune. The upper bound on $R$ must thus be as tight as reasonably possible. The bound we maintain throughout the calculation is a minimum of several bounds, that come in three categories. The first category of bounds are those intrinsic to the rules themselves. This category includes bounds stating that each rule must capture sufficient data; if not, the rule list is provably non-optimal. The second type of bound compares an upper bound on the value of $R$ to that of the current best solution. This allows us to exclude parts of the search space that could never be better than our current solution. Finally, our last type of bound is based on comparing incomplete rule lists that capture the same data and pursuing only the more accurate option. This last class of bounds is especially important -- without our use of a novel \textit{symmetry-aware map}, we are unable to solve most problems of reasonable scale. This symmetry-aware map keeps track of the best accuracy for all the permutations of a given incomplete rule list.

We evaluated BBRL??? on a number of publicly available datasets, and have made code for our algorithm and experiments publicly available. Our metric of success was 10-fold cross validated prediction accuracy on a subset of the data. These datasets involve hundreds of rules and hundreds or thousands of observations. BBRL??? is generally able to find the optimal rule list in a matter of seconds and certify it within about 10 minutes. We show that we are able to achieve better out-of-sample accuracy on these datasets than the popular greedy algorithms, CART or C5.0.

This algorithm is designed for solving large (not massive) problems, where interpretability and certifiable optimality is important. A key example of the type of problems this algorithm is useful for is recidivism prediction. Thus, we work on the dataset released by Northpointe, who designed the proprietary COMPAS algorithm. We show that, at least on this sample of the data, it is possible to produce an certifiably optimal interpretable rule list that achieves the same accuracy as a random forest applied to this dataset. We thus see no reason why a proprietary algorithm should be used for recidivism prediction.


\end{document}