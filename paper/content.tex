\section{Introduction}

\emph{Rule lists}, also called decision lists, are one-sided decision trees.

\section{Related work}

\citep{rivest:1987}

\citep{LethamRuMcMa15}

\citep{YangRuSe16}

\section{A branch-and-bound framework for optimizing rule lists}

\subsection{Rule lists for binary classification}

We restrict our setting to binary classification,
where rule lists are Boolean functions;
this framework is straightforward to generalize to multi-class classification.
%
Let us denote training data by~${\{(x_n, y_n)\}_{n=1}^N}$,
where~${x_n \in \{0, 1\}^d}$ are binary features and~${y_n \in \{0, 1\}}$ are labels.

A rule list~$\RL$ of length~$k \ge 0$ is defined by a $(k+1)$-tuple consisting of~$k$
association rules followed by a default rule~$d$.
%
An association rule~${p \rightarrow q}$ is an implication corresponding to the
conditional statement, ``if~$p$, then~$q$.''
%
In our setting, an antecedent~$p$ is a Boolean assertion that evaluates to either
true or false for each datum~$x_n$, and a consequent~$q$ is a label prediction.
%
For example,~${(x_{i, 1} = 0) \wedge (x_{i, 3} = 1) \rightarrow (y_i = 1)}$
is an association rule that could appear in a rule list.
%
The number of conditions in an antecedent is its cardinality;
the antecedent in the previous example has a cardinality of two.
%
The final default rule~$d$ in a rule list can be thought of as a special
association rule whose antecedent simply asserts true.

A rule list classifies a datum~$x_n$ by providing the predicted label given
by the consequent of the first rule whose antecedent is true for~$x_n$.
%
We say that this rule captures~$x_n$ in the context of the rule list;
note that this rule can be the default rule.
%
Let a rule list's prefix~$\Prefix$ be a $k$-tuple of its antecedents.
%
We say that a prefix~$\Prefix'$ of length~$k'$ starts with prefix~$\Prefix$ of
length~$k \le k'$ if the first~$k$ association rules of~$\Prefix'$ are given by~$\Prefix$.

Finally, given training data, a prefix of antecedents implies a rule list
whose label predictions of the corresponding consequents and of the default rule
are empirically determined to minimize the number misclassification errors made
by the rule list on the training data.
%
In the remainder of our presentation, whenever we refer to a rule list with a
particular prefix, we implicitly assume these empirically determined label predictions.

\subsection{Branch-and-bound optimization framework}

We define a simple loss function for a rule list~$\RL$ of length~$k$:
\begin{align}
\ell(\RL) = m(\RL) + c k.
\label{eq:objective}
\end{align}
The two terms correspond to the misclassification error~$m(\RL)$
and a regularization term that penalizes longer rule lists.
%
$m(\RL)$~is the fraction of training data whose labels are
incorrectly predicted by~$\RL$.
%
In our setting, the regularization parameter~$c$ is a small constant;
\eg ${c = 0.01}$ can be thought of as adding a penalty equivalent to misclassifying~$1\%$
of data when increasing a decision rule's length by one association rule.

Our objective has structure amenable to global optimization via a branch-and-bound framework.
%
In particular, we can decompose the misclassification error into two contributions, 
\begin{align}
m(\RL) = m(\Prefix) + m(d),
\end{align}
corresponding to mistakes made by the prefix and default rule, respectively.
%
Eliminating the latter error term gives a lower bound~$b(\Prefix)$ on the objective,
\begin{align}
b(\Prefix) \equiv m(\Prefix) + c k \le \ell(\RL),
\label{eq:lower-bound}
\end{align}
which is actually a lower bound on the objective of \emph{any} rule list
of length~$k$ or longer whose prefix starts with~$\Prefix$.
%
Notice that if a prefix~$\Prefix'$ starts with~$\Prefix$, then~${m(\Prefix') \ge m(\Prefix)}$
because~$\Prefix'$ makes the same mistakes as~$\Prefix$, and possibly additional mistakes.
%
Therefore,~${b(\Prefix') \ge b(\Prefix)}$.
%
A sequence of rule lists formed by sequentially appending association rules
to the end of a prefix thus has a corresponding sequence of
monotonically increasing objective lower bounds.
%
This is precisely the structure required and exploited by branch-and-bound.

Specifically, the lower bound tells us when we can prune the state space.
%
While executing branch-and-bound, we keep track of the best (smallest) known objective~$\ell*$.
%
If we encounter a prefix~$\Prefix$ with lower bound at least as large as~$\ell*$,
\begin{align}
b(\Prefix) \ge \ell*,
\end{align}
then we needn't consider any prefix that starts with~$\Prefix$.

\subsection{Additional pruning bounds}

Our problem has rich structure beyond the lower bound in~\eqref{eq:lower-bound}.
%
In this section, we enumerate a series of additional bounds that combine to yield
aggressive pruning opportunities throughout the execution of our branch-and-bound algorithm.

\subsubsection{Lower bound with one-step lookahead}

We begin with an immediate consequence of~\eqref{eq:lower-bound}.
%
Given a prefix~$\Prefix$ of length~$k$, any longer prefix~$\Prefix'$ that starts
with~$\Prefix$ has length at least~${k+1}$.
%
Thus, even if we find that~${b(\Prefix) < \ell*}$, we can still prune all longer
prefixes~$\Prefix'$ if
\begin{align}
b(\Prefix) + c = m(\Prefix) + c (k + 1) \ge \ell*,
\end{align}
where $\ell*$ is the best known objective.

\subsubsection{Permutation bound for symmetry-aware garbage collection}

Let us say that a prefix of length~$k$ captures those data that are captured
by one of its~$k$ association rules; for a rule list with prefix~$\Prefix$,
data not captured by~$\Prefix$ are classified by the default rule.
%
If two prefixes~$\cal{P}$ and~$\cal{Q}$ are composed of the same antecedents and
equivalent up to a permutation, then they also capture the same data.
%
Their two corresponding rule lists need not yield the same objective, since the
loss function depends on rule order.
%
Obtain a prefix~$\cal{P}'$ by appending~$\cal{P}$ with some ordered list of
unique antecedents not contained in~$\cal{P}$, and~$\cal{Q}'$ by appending~$\cal{Q}$
with the same ordered list.
%
The performance of the rule list formed from~$\cal{P}'$ compared to~$\cal{P}$ will be
the same as that of the rule list formed from~$\cal{Q}'$ compared to~$\cal{Q}$, \eg
\begin{align}
\ell(\cal{P}') - \ell(\cal{P}) = \ell(\cal{Q}') - \ell(\cal{Q}).
\end{align}
Thus, we can prune whichever of~$\cal{P}$ or~$\cal{Q}$ performs worse than the other.
%
We call this symmetry-aware garbage collection.
%
Since a prefix of length~$k$ belongs to an equivalence class of~$k!$ prefixes
equivalent up to permutation, this garbage collection dramatically prunes the search space.

\subsubsection{Not captured bound}

Generalization of captures.

\begin{itemize}
\item other permutation bounds
\item similarity bounds
\item support positive bound
\item not-too-many incorrect bound
\item not too similar to the default rule bound
\item when rules commute, dominates
\item when a rule has too-low support after a prefix, then it will always have too-low support below the prefix
\end{itemize}

\subsection{Cache data structure}
\label{sec:cache}

Our cache is a trie.

\subsection{Symmetry-aware data structure}

\subsection{Scheduling policies}

\begin{itemize}
\item breadth-first
\item depth-first
\item something based on greedy
\item (curiosity, lower bound, optimization) $\times$ (priority queue, something like Thompson sampling)
\item optimistic
\end{itemize}

\subsection{Large-scale optimization}

\subsection{System}

\section{Experiments}

\begin{itemize}

\item Value of the objective for us and a few other algorithms (CART, C4.5, CBA, CMAR/CPAR, C5.0 \dots), and algorithm runtimes 

\item Show the effect of each ``piece'' at a time, \eg curiosity -- run it without each in turn and show the difference in either quality of solution or runtime or amount of memory, size of cache or queue

\item Fraction of search space eliminated over time

\item Size of queue over time

\item Total number of things placed in queue (over time?)

\item Objective function value over time -- this also gives an upper bound on the remaining search space -- and also, possibly, the minimum lower bound in the queue over time

\item Some characterization of the number of solutions close to optimal (plot number of suboptimal solutions vs amount of suboptimality, removing permutations)

\item Some example rule lists to show how interpretable they are. Potentially we could display equally optimal rule lists that look very different from each other. 

\end{itemize}

\section{Conclusions}

\subsubsection*{Acknowledgments}

E.A. is supported by the Miller Institute for Basic Research in Science, University of California, Berkeley.

\bibliography{refs}
\bibliographystyle{abbrvnat}
