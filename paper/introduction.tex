\documentclass[aoas,preprint]{imsart}
\usepackage{fullpage}
\setattribute{journal}{name}{}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}

\usepackage{graphicx,verbatim}
\usepackage[round]{natbib}
\usepackage{url}
\usepackage{amsmath,amssymb,amsthm,amsfonts}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{todonotes}
\usepackage{subfig}
\usepackage{dsfont}
\usepackage{listings}
\usepackage{comment}

\usepackage{tikz}
\usetikzlibrary{arrows}

\input{latex_macros}
\frenchspacing
\hyphenation{speed-up}

\begin{document}

\section{Introduction}

As machine learning continues to grow in importance, the interpretability of
predictive models remains a crucial problem.
%
Our aim is to build models that are highly predictive but in which each step in
the model's decision making process can also be investigated by humans.
%
To achieve this, we use \emph{rule lists}, also known as decision lists, which
are lists composed of \emph{if-then} statements.
%
This structure allows for predictive models that also can be easily interpreted
because the rules that are satisfied give a reason for each prediction.
%
Given a set of rules associated with a dataset, rule lists can be constructed by
placing the rules in different orderings.
%
Since most data points fall under multiple, changing the ordering of rules leads
to different predictions and therefore different accuracies.
%
The goal is to maximize predictive accuracy through the discovery of different rules lists.
%
A brute force solution to find the rule list with the highest accuracy,
the optimal rule list, would be computationally prohibitive
due to the exponential number of rule lists.
%
Our goal is to create an algorithm that could find the optimal rule list
in a reasonable amount of time.

Recent advances in the field of rule lists such as~\citep{LethamRuMcMa15}
and~\citep{YangRuSe16} have focused on probabilistic approaches to generating rule lists.
%
These approaches achieve high accuracy while also managing to run quickly.
%
However, despite the apparent accuracy of the rule lists generated by these algorithms,
there is no way to determine if the generated rule list is optimal or
how close to optimal the rule lists is.
%
Our model, called Branch and Bound Rule Lists (BBRL), finds the optimal rule list
and allows us to also investigate the accuracy of near optimal solutions.
%
The benefits of this model are two-fold: firstly, we are able to generate
the best rule list on a given data set and therefore will have
the most accurate predictions that a rule list can give.
%
Secondly, since BBRL generates the entire space of potential solutions,
we can judge how good rule lists generated by other algorithms are.
%
In particular, we can investigate if the rule lists from probabilistic approaches
are nearly optimal or whether those approaches sacrifice too much accuracy for speed.

BBRL achieves these results by bounding the maximum accuracy that a rule list
can achieve in the future.
%
This allows us to prune that rule list if the bound is worse than the accuracy
of the best rule list that we've already looked at.
%
We continue to look at rule lists until we have either looked at every rule list
or eliminated it from consideration.
%
Thus, the end of execution leaves us with the optimal rule list.
%
Our use of the branch and bound technique leads to massive pruning of the search space of
potential rule lists and means our algorithm can find the optimal rule list on real data sets.

We evaluated BBRL on a number of datasets from the UCI repository.
%
Our metric of success was prediction accuracy on a subset of the data which
we calculated using 10-fold cross validation.
%
These datasets involve hundreds of rules and hundreds or thousands of samples
and BBRL is able to find the optimal rule list within 10 minutes.
%
We show that we are able to achieve better accuracy on these datasets than
the popular greedy algorithms, CART or C5.0.

Due to the exponential nature of the problem, the efficacy of BBRL is dependent
on how much our bounds allow us to prune.
%
In this paper we list a few types of bounds that allow us to drastically prune our search space.
%
The first type of bound is intrinsic to the rules themselves.
%
This category includes bounds such as ensuring that rules capture enough data
correctly to overcome a regularization parameter.
%
Our second type of bound is dependent on the accuracy of the current best solution.
%
This allows us to not examine parts of the search space that could never be
better than our current solution.
%
Finally, our last class of bounds is based on comparing rule lists that capture
the same data and only pursuing the more accurate option.
%
This last class of bounds is especially important--without our use of a novel
symmetry-aware map, we are unable to solve any problems of reasonable scale.
%
This symmetry-aware map keeps track of the best accuracy for all the permutations of a given prefix.

BBRL demonstrates a novel approach towards generating interpretable models by
looking for the optimal rule list.
%
While searching for that optimal list, we are able to discover near-optimal
solutions that provide insight into how effective other interpretable methods might be.
%
We have proved numerous bounds that allow us to prune rule lists.
%
Finally, we created a novel symmetry-aware map that drastically reduces the
search space of possible rule lists.

\end{document}